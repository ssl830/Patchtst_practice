{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 50,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.14327751100063324,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.3226,
      "step": 10
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.10806774348020554,
      "learning_rate": 9.873333333333334e-06,
      "loss": 0.374,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.19044993817806244,
      "learning_rate": 9.806666666666667e-06,
      "loss": 0.3681,
      "step": 30
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.03453294560313225,
      "learning_rate": 9.74e-06,
      "loss": 0.3414,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.1488494724035263,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.35,
      "step": 50
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.17486350238323212,
      "learning_rate": 9.606666666666667e-06,
      "loss": 0.3215,
      "step": 60
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.09923525899648666,
      "learning_rate": 9.54e-06,
      "loss": 0.3385,
      "step": 70
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.07761720567941666,
      "learning_rate": 9.473333333333335e-06,
      "loss": 0.3404,
      "step": 80
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.24804002046585083,
      "learning_rate": 9.406666666666668e-06,
      "loss": 0.3542,
      "step": 90
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.1942584216594696,
      "learning_rate": 9.340000000000002e-06,
      "loss": 0.346,
      "step": 100
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.13700810074806213,
      "learning_rate": 9.273333333333335e-06,
      "loss": 0.2944,
      "step": 110
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.2590557038784027,
      "learning_rate": 9.206666666666668e-06,
      "loss": 0.3444,
      "step": 120
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.5413126945495605,
      "learning_rate": 9.14e-06,
      "loss": 0.3173,
      "step": 130
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.5250876545906067,
      "learning_rate": 9.073333333333333e-06,
      "loss": 0.2787,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.27632206678390503,
      "learning_rate": 9.006666666666666e-06,
      "loss": 0.2912,
      "step": 150
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.8034558892250061,
      "learning_rate": 8.94e-06,
      "loss": 0.3038,
      "step": 160
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.36665844917297363,
      "learning_rate": 8.873333333333334e-06,
      "loss": 0.3271,
      "step": 170
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.8336086273193359,
      "learning_rate": 8.806666666666668e-06,
      "loss": 0.2818,
      "step": 180
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.9433474540710449,
      "learning_rate": 8.740000000000001e-06,
      "loss": 0.2766,
      "step": 190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.6698334217071533,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.3043,
      "step": 200
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.2908130884170532,
      "learning_rate": 8.606666666666668e-06,
      "loss": 0.2682,
      "step": 210
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.7297230362892151,
      "learning_rate": 8.540000000000001e-06,
      "loss": 0.2421,
      "step": 220
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.18501058220863342,
      "learning_rate": 8.473333333333334e-06,
      "loss": 0.2483,
      "step": 230
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.555727481842041,
      "learning_rate": 8.406666666666667e-06,
      "loss": 0.2688,
      "step": 240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.818037748336792,
      "learning_rate": 8.34e-06,
      "loss": 0.2807,
      "step": 250
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 2.5119194984436035,
      "learning_rate": 8.273333333333334e-06,
      "loss": 0.2684,
      "step": 260
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.1674970388412476,
      "learning_rate": 8.206666666666667e-06,
      "loss": 0.2738,
      "step": 270
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.32598310708999634,
      "learning_rate": 8.14e-06,
      "loss": 0.2525,
      "step": 280
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 1.9649999141693115,
      "learning_rate": 8.073333333333335e-06,
      "loss": 0.2822,
      "step": 290
    },
    {
      "epoch": 0.4,
      "grad_norm": 4.429129123687744,
      "learning_rate": 8.006666666666667e-06,
      "loss": 0.2699,
      "step": 300
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 2.5149850845336914,
      "learning_rate": 7.94e-06,
      "loss": 0.2435,
      "step": 310
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.8924759030342102,
      "learning_rate": 7.873333333333335e-06,
      "loss": 0.263,
      "step": 320
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.0136218070983887,
      "learning_rate": 7.806666666666668e-06,
      "loss": 0.2768,
      "step": 330
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 1.3250216245651245,
      "learning_rate": 7.74e-06,
      "loss": 0.256,
      "step": 340
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.9471489191055298,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.2639,
      "step": 350
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.7106188535690308,
      "learning_rate": 7.606666666666668e-06,
      "loss": 0.2806,
      "step": 360
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 1.0920389890670776,
      "learning_rate": 7.540000000000001e-06,
      "loss": 0.2606,
      "step": 370
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 1.3753941059112549,
      "learning_rate": 7.4733333333333335e-06,
      "loss": 0.2508,
      "step": 380
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.3567445278167725,
      "learning_rate": 7.406666666666667e-06,
      "loss": 0.2624,
      "step": 390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.7165417075157166,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.2579,
      "step": 400
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.1704325675964355,
      "learning_rate": 7.2733333333333346e-06,
      "loss": 0.2345,
      "step": 410
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.9435151815414429,
      "learning_rate": 7.206666666666667e-06,
      "loss": 0.2433,
      "step": 420
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 2.772170066833496,
      "learning_rate": 7.14e-06,
      "loss": 0.2509,
      "step": 430
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.086451530456543,
      "learning_rate": 7.073333333333334e-06,
      "loss": 0.2639,
      "step": 440
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.501923084259033,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.2562,
      "step": 450
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 2.7352511882781982,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 0.2581,
      "step": 460
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 3.011115074157715,
      "learning_rate": 6.873333333333334e-06,
      "loss": 0.2553,
      "step": 470
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.240172863006592,
      "learning_rate": 6.806666666666667e-06,
      "loss": 0.2469,
      "step": 480
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 10.505352973937988,
      "learning_rate": 6.740000000000001e-06,
      "loss": 0.272,
      "step": 490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 5.1187615394592285,
      "learning_rate": 6.6733333333333335e-06,
      "loss": 0.2192,
      "step": 500
    },
    {
      "epoch": 0.68,
      "grad_norm": 6.0946221351623535,
      "learning_rate": 6.606666666666666e-06,
      "loss": 0.2547,
      "step": 510
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 1.816387414932251,
      "learning_rate": 6.540000000000001e-06,
      "loss": 0.2499,
      "step": 520
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 4.712549686431885,
      "learning_rate": 6.473333333333334e-06,
      "loss": 0.2379,
      "step": 530
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.4452109336853027,
      "learning_rate": 6.4066666666666674e-06,
      "loss": 0.252,
      "step": 540
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.3395140171051025,
      "learning_rate": 6.34e-06,
      "loss": 0.2561,
      "step": 550
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.9085402488708496,
      "learning_rate": 6.273333333333333e-06,
      "loss": 0.2476,
      "step": 560
    },
    {
      "epoch": 0.76,
      "grad_norm": 3.379340171813965,
      "learning_rate": 6.206666666666668e-06,
      "loss": 0.2576,
      "step": 570
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.7689846158027649,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 0.2356,
      "step": 580
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 11.459124565124512,
      "learning_rate": 6.073333333333333e-06,
      "loss": 0.2395,
      "step": 590
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.3759841918945312,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.2606,
      "step": 600
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 3.6967954635620117,
      "learning_rate": 5.94e-06,
      "loss": 0.2305,
      "step": 610
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.9971333742141724,
      "learning_rate": 5.873333333333334e-06,
      "loss": 0.2601,
      "step": 620
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.108076572418213,
      "learning_rate": 5.806666666666667e-06,
      "loss": 0.2491,
      "step": 630
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 3.448526382446289,
      "learning_rate": 5.74e-06,
      "loss": 0.2334,
      "step": 640
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 1.919861912727356,
      "learning_rate": 5.673333333333334e-06,
      "loss": 0.2281,
      "step": 650
    },
    {
      "epoch": 0.88,
      "grad_norm": 3.165240526199341,
      "learning_rate": 5.606666666666667e-06,
      "loss": 0.2361,
      "step": 660
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 2.5210227966308594,
      "learning_rate": 5.540000000000001e-06,
      "loss": 0.2245,
      "step": 670
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.5991036891937256,
      "learning_rate": 5.473333333333334e-06,
      "loss": 0.2328,
      "step": 680
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.6168370246887207,
      "learning_rate": 5.406666666666667e-06,
      "loss": 0.2575,
      "step": 690
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.1224842071533203,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.2567,
      "step": 700
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 3.9069011211395264,
      "learning_rate": 5.273333333333333e-06,
      "loss": 0.2316,
      "step": 710
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.381252288818359,
      "learning_rate": 5.206666666666668e-06,
      "loss": 0.2646,
      "step": 720
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 3.7818615436553955,
      "learning_rate": 5.140000000000001e-06,
      "loss": 0.2444,
      "step": 730
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 1.0965266227722168,
      "learning_rate": 5.073333333333334e-06,
      "loss": 0.2543,
      "step": 740
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.5853557586669922,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.2363,
      "step": 750
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 1.9343172311782837,
      "learning_rate": 4.94e-06,
      "loss": 0.2413,
      "step": 760
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 4.438992023468018,
      "learning_rate": 4.873333333333334e-06,
      "loss": 0.23,
      "step": 770
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.3249714374542236,
      "learning_rate": 4.8066666666666675e-06,
      "loss": 0.2531,
      "step": 780
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 2.2632627487182617,
      "learning_rate": 4.74e-06,
      "loss": 0.2401,
      "step": 790
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 2.221144437789917,
      "learning_rate": 4.673333333333333e-06,
      "loss": 0.235,
      "step": 800
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.746762752532959,
      "learning_rate": 4.606666666666667e-06,
      "loss": 0.2373,
      "step": 810
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 4.521313190460205,
      "learning_rate": 4.540000000000001e-06,
      "loss": 0.2387,
      "step": 820
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 2.7121376991271973,
      "learning_rate": 4.473333333333334e-06,
      "loss": 0.2342,
      "step": 830
    },
    {
      "epoch": 1.12,
      "grad_norm": 6.342206954956055,
      "learning_rate": 4.406666666666667e-06,
      "loss": 0.224,
      "step": 840
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 2.9141459465026855,
      "learning_rate": 4.34e-06,
      "loss": 0.2448,
      "step": 850
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 1.3487656116485596,
      "learning_rate": 4.273333333333334e-06,
      "loss": 0.2536,
      "step": 860
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.848361849784851,
      "learning_rate": 4.206666666666667e-06,
      "loss": 0.2346,
      "step": 870
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 6.538373947143555,
      "learning_rate": 4.14e-06,
      "loss": 0.2484,
      "step": 880
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 6.7115020751953125,
      "learning_rate": 4.073333333333334e-06,
      "loss": 0.2431,
      "step": 890
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.3882548809051514,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.2623,
      "step": 900
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 5.662164688110352,
      "learning_rate": 3.94e-06,
      "loss": 0.2277,
      "step": 910
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 1.5863091945648193,
      "learning_rate": 3.873333333333333e-06,
      "loss": 0.2425,
      "step": 920
    },
    {
      "epoch": 1.24,
      "grad_norm": 3.4602761268615723,
      "learning_rate": 3.806666666666667e-06,
      "loss": 0.2354,
      "step": 930
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 2.2012484073638916,
      "learning_rate": 3.74e-06,
      "loss": 0.2334,
      "step": 940
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 2.2722630500793457,
      "learning_rate": 3.673333333333334e-06,
      "loss": 0.2514,
      "step": 950
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.5383102893829346,
      "learning_rate": 3.606666666666667e-06,
      "loss": 0.2297,
      "step": 960
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.5903561115264893,
      "learning_rate": 3.54e-06,
      "loss": 0.2323,
      "step": 970
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 3.17655873298645,
      "learning_rate": 3.4733333333333337e-06,
      "loss": 0.236,
      "step": 980
    },
    {
      "epoch": 1.32,
      "grad_norm": 7.046579360961914,
      "learning_rate": 3.406666666666667e-06,
      "loss": 0.2406,
      "step": 990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.6628804206848145,
      "learning_rate": 3.3400000000000006e-06,
      "loss": 0.2259,
      "step": 1000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 5.588246822357178,
      "learning_rate": 3.2733333333333335e-06,
      "loss": 0.2458,
      "step": 1010
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.994896411895752,
      "learning_rate": 3.2066666666666667e-06,
      "loss": 0.2567,
      "step": 1020
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 7.9031982421875,
      "learning_rate": 3.1400000000000004e-06,
      "loss": 0.2365,
      "step": 1030
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 2.0572702884674072,
      "learning_rate": 3.0733333333333337e-06,
      "loss": 0.2286,
      "step": 1040
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.595365524291992,
      "learning_rate": 3.0066666666666674e-06,
      "loss": 0.2505,
      "step": 1050
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 2.247386932373047,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 0.2287,
      "step": 1060
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 2.013498067855835,
      "learning_rate": 2.8733333333333335e-06,
      "loss": 0.2468,
      "step": 1070
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.2996127605438232,
      "learning_rate": 2.806666666666667e-06,
      "loss": 0.2426,
      "step": 1080
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 4.536278247833252,
      "learning_rate": 2.7400000000000004e-06,
      "loss": 0.2331,
      "step": 1090
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2.102294683456421,
      "learning_rate": 2.6733333333333333e-06,
      "loss": 0.2297,
      "step": 1100
    },
    {
      "epoch": 1.48,
      "grad_norm": 5.999453067779541,
      "learning_rate": 2.606666666666667e-06,
      "loss": 0.2372,
      "step": 1110
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 1.1516979932785034,
      "learning_rate": 2.5400000000000002e-06,
      "loss": 0.2539,
      "step": 1120
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 5.144167900085449,
      "learning_rate": 2.4733333333333335e-06,
      "loss": 0.2361,
      "step": 1130
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.7294373512268066,
      "learning_rate": 2.4066666666666668e-06,
      "loss": 0.2588,
      "step": 1140
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 8.922061920166016,
      "learning_rate": 2.3400000000000005e-06,
      "loss": 0.2447,
      "step": 1150
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 4.212348937988281,
      "learning_rate": 2.2733333333333333e-06,
      "loss": 0.2374,
      "step": 1160
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.8157663345336914,
      "learning_rate": 2.206666666666667e-06,
      "loss": 0.2122,
      "step": 1170
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 5.3662848472595215,
      "learning_rate": 2.1400000000000003e-06,
      "loss": 0.2324,
      "step": 1180
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 2.8977882862091064,
      "learning_rate": 2.0733333333333335e-06,
      "loss": 0.2219,
      "step": 1190
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.5929794311523438,
      "learning_rate": 2.006666666666667e-06,
      "loss": 0.2517,
      "step": 1200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 1.4679478406906128,
      "learning_rate": 1.94e-06,
      "loss": 0.2364,
      "step": 1210
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 7.895040035247803,
      "learning_rate": 1.8733333333333333e-06,
      "loss": 0.2518,
      "step": 1220
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 4.219503402709961,
      "learning_rate": 1.8066666666666668e-06,
      "loss": 0.2601,
      "step": 1230
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 3.169846296310425,
      "learning_rate": 1.74e-06,
      "loss": 0.2379,
      "step": 1240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.4563974142074585,
      "learning_rate": 1.6733333333333335e-06,
      "loss": 0.2367,
      "step": 1250
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 2.410662889480591,
      "learning_rate": 1.606666666666667e-06,
      "loss": 0.252,
      "step": 1260
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 3.667372465133667,
      "learning_rate": 1.54e-06,
      "loss": 0.2339,
      "step": 1270
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 2.035858392715454,
      "learning_rate": 1.4733333333333336e-06,
      "loss": 0.2333,
      "step": 1280
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.2032217979431152,
      "learning_rate": 1.4066666666666668e-06,
      "loss": 0.2264,
      "step": 1290
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.7762784957885742,
      "learning_rate": 1.34e-06,
      "loss": 0.2324,
      "step": 1300
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 3.947080612182617,
      "learning_rate": 1.2733333333333334e-06,
      "loss": 0.2371,
      "step": 1310
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.2019455432891846,
      "learning_rate": 1.2066666666666668e-06,
      "loss": 0.2426,
      "step": 1320
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 2.0375654697418213,
      "learning_rate": 1.14e-06,
      "loss": 0.2304,
      "step": 1330
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 4.624685764312744,
      "learning_rate": 1.0733333333333334e-06,
      "loss": 0.2438,
      "step": 1340
    },
    {
      "epoch": 1.8,
      "grad_norm": 4.901003837585449,
      "learning_rate": 1.0066666666666668e-06,
      "loss": 0.2395,
      "step": 1350
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 6.721760272979736,
      "learning_rate": 9.400000000000001e-07,
      "loss": 0.2459,
      "step": 1360
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.9147775173187256,
      "learning_rate": 8.733333333333334e-07,
      "loss": 0.2447,
      "step": 1370
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 3.9432106018066406,
      "learning_rate": 8.066666666666667e-07,
      "loss": 0.237,
      "step": 1380
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 3.0456607341766357,
      "learning_rate": 7.4e-07,
      "loss": 0.2542,
      "step": 1390
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 4.756011009216309,
      "learning_rate": 6.733333333333334e-07,
      "loss": 0.2329,
      "step": 1400
    },
    {
      "epoch": 1.88,
      "grad_norm": 3.177159547805786,
      "learning_rate": 6.066666666666668e-07,
      "loss": 0.2185,
      "step": 1410
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 3.6954939365386963,
      "learning_rate": 5.4e-07,
      "loss": 0.2276,
      "step": 1420
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 6.116954803466797,
      "learning_rate": 4.7333333333333334e-07,
      "loss": 0.2434,
      "step": 1430
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.7202200293540955,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.2233,
      "step": 1440
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 5.073678970336914,
      "learning_rate": 3.4000000000000003e-07,
      "loss": 0.2279,
      "step": 1450
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 5.3538384437561035,
      "learning_rate": 2.7333333333333335e-07,
      "loss": 0.2249,
      "step": 1460
    },
    {
      "epoch": 1.96,
      "grad_norm": 3.1000864505767822,
      "learning_rate": 2.066666666666667e-07,
      "loss": 0.2443,
      "step": 1470
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 2.9934749603271484,
      "learning_rate": 1.4e-07,
      "loss": 0.2328,
      "step": 1480
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 2.8583133220672607,
      "learning_rate": 7.333333333333334e-08,
      "loss": 0.2502,
      "step": 1490
    },
    {
      "epoch": 2.0,
      "grad_norm": 10.013215065002441,
      "learning_rate": 6.666666666666667e-09,
      "loss": 0.24,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 13502073600000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
