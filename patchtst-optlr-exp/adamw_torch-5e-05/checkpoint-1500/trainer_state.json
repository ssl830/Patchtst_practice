{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 50,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.07426663488149643,
      "learning_rate": 4.97e-05,
      "loss": 0.3211,
      "step": 10
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.170313760638237,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.3677,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.42634904384613037,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.3477,
      "step": 30
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.16092611849308014,
      "learning_rate": 4.87e-05,
      "loss": 0.3193,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.8075681924819946,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.2994,
      "step": 50
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.4854934811592102,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.26,
      "step": 60
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.9890381693840027,
      "learning_rate": 4.77e-05,
      "loss": 0.2662,
      "step": 70
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.6160570979118347,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.2564,
      "step": 80
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.7679433822631836,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.2696,
      "step": 90
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.6478334069252014,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.2532,
      "step": 100
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.8061051368713379,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.25,
      "step": 110
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.9698135852813721,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.2597,
      "step": 120
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 1.307036280632019,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.2647,
      "step": 130
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 1.87750244140625,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.2339,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.9192882776260376,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.2653,
      "step": 150
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.7407742142677307,
      "learning_rate": 4.47e-05,
      "loss": 0.2411,
      "step": 160
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.6542708277702332,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.2509,
      "step": 170
    },
    {
      "epoch": 0.24,
      "grad_norm": 5.032162666320801,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.238,
      "step": 180
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.5826337337493896,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.237,
      "step": 190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.7731592655181885,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.2451,
      "step": 200
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.5497126579284668,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.2241,
      "step": 210
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.41580432653427124,
      "learning_rate": 4.27e-05,
      "loss": 0.2212,
      "step": 220
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.4200827479362488,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.2322,
      "step": 230
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.558443307876587,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.2435,
      "step": 240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.154982566833496,
      "learning_rate": 4.17e-05,
      "loss": 0.2369,
      "step": 250
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 2.2097883224487305,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.2286,
      "step": 260
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.7429166436195374,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.2449,
      "step": 270
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.056877613067627,
      "learning_rate": 4.07e-05,
      "loss": 0.2179,
      "step": 280
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 1.2850100994110107,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.258,
      "step": 290
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.266251802444458,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.2418,
      "step": 300
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.2803847789764404,
      "learning_rate": 3.97e-05,
      "loss": 0.2238,
      "step": 310
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.7182691097259521,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.241,
      "step": 320
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.1669979095458984,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.2571,
      "step": 330
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.43167635798454285,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.2305,
      "step": 340
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.675506591796875,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.2443,
      "step": 350
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.9159488677978516,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.2584,
      "step": 360
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 1.2044742107391357,
      "learning_rate": 3.77e-05,
      "loss": 0.2408,
      "step": 370
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 4.19489049911499,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.2232,
      "step": 380
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.5886458158493042,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.2403,
      "step": 390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.9158414006233215,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.2407,
      "step": 400
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 3.154139757156372,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.2241,
      "step": 410
    },
    {
      "epoch": 0.56,
      "grad_norm": 2.14412784576416,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.2294,
      "step": 420
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 4.698596954345703,
      "learning_rate": 3.57e-05,
      "loss": 0.2332,
      "step": 430
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.6539660692214966,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.2489,
      "step": 440
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.0923802852630615,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.2388,
      "step": 450
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 1.6211731433868408,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.2469,
      "step": 460
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 1.1258269548416138,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.2451,
      "step": 470
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.8864929676055908,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.2339,
      "step": 480
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 9.009472846984863,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.2532,
      "step": 490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.3562545776367188,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.2121,
      "step": 500
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.360387921333313,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.2377,
      "step": 510
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.3883100152015686,
      "learning_rate": 3.27e-05,
      "loss": 0.2357,
      "step": 520
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.233644962310791,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.2239,
      "step": 530
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.7661788463592529,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.2329,
      "step": 540
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.7817755937576294,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.2346,
      "step": 550
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 3.2204439640045166,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.2365,
      "step": 560
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.916269063949585,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.2473,
      "step": 570
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.6160321235656738,
      "learning_rate": 3.07e-05,
      "loss": 0.2239,
      "step": 580
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 8.37360668182373,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.2276,
      "step": 590
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.050899624824524,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.2416,
      "step": 600
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 1.8120527267456055,
      "learning_rate": 2.97e-05,
      "loss": 0.218,
      "step": 610
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 2.4661600589752197,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.2441,
      "step": 620
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.32598215341567993,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.2314,
      "step": 630
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 1.5928304195404053,
      "learning_rate": 2.87e-05,
      "loss": 0.2207,
      "step": 640
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.712071418762207,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.2223,
      "step": 650
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.295048236846924,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.2377,
      "step": 660
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 1.770888090133667,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.2167,
      "step": 670
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.1805481910705566,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.2255,
      "step": 680
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.5617904663085938,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.2468,
      "step": 690
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.7650201916694641,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.2402,
      "step": 700
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.8268930912017822,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.2242,
      "step": 710
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.374560832977295,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.2388,
      "step": 720
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 1.7248607873916626,
      "learning_rate": 2.57e-05,
      "loss": 0.2305,
      "step": 730
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.4921112060546875,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.2436,
      "step": 740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7867779731750488,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.2231,
      "step": 750
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 2.9631288051605225,
      "learning_rate": 2.47e-05,
      "loss": 0.2281,
      "step": 760
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 3.5056467056274414,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.2211,
      "step": 770
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.159008741378784,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.2367,
      "step": 780
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.8573735356330872,
      "learning_rate": 2.37e-05,
      "loss": 0.2342,
      "step": 790
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.8151022791862488,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.2215,
      "step": 800
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.6593186855316162,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.2347,
      "step": 810
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1.1709378957748413,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.2229,
      "step": 820
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 2.798903226852417,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.2257,
      "step": 830
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.8081310987472534,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.2139,
      "step": 840
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.6373426914215088,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.2305,
      "step": 850
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 1.7576824426651,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.2354,
      "step": 860
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.3773456811904907,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.2214,
      "step": 870
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 7.01357889175415,
      "learning_rate": 2.07e-05,
      "loss": 0.2444,
      "step": 880
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 13.084819793701172,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.2262,
      "step": 890
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.2866328954696655,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.2478,
      "step": 900
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 3.4400079250335693,
      "learning_rate": 1.97e-05,
      "loss": 0.2137,
      "step": 910
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.7287680506706238,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.235,
      "step": 920
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.5601563453674316,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.2265,
      "step": 930
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 4.837747573852539,
      "learning_rate": 1.87e-05,
      "loss": 0.2254,
      "step": 940
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.871808648109436,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.2384,
      "step": 950
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.1722551584243774,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.2257,
      "step": 960
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.6160663366317749,
      "learning_rate": 1.77e-05,
      "loss": 0.2277,
      "step": 970
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 2.9871528148651123,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.2258,
      "step": 980
    },
    {
      "epoch": 1.32,
      "grad_norm": 8.421454429626465,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.2268,
      "step": 990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.4257073402404785,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.22,
      "step": 1000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 6.262970924377441,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.2364,
      "step": 1010
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.6328694820404053,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.2443,
      "step": 1020
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 4.9675068855285645,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.2259,
      "step": 1030
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.7424358129501343,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.2236,
      "step": 1040
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.389817953109741,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.2346,
      "step": 1050
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.6095300912857056,
      "learning_rate": 1.47e-05,
      "loss": 0.2156,
      "step": 1060
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 1.043077826499939,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.2315,
      "step": 1070
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.122802734375,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.2271,
      "step": 1080
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 3.3777425289154053,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.2254,
      "step": 1090
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.261450171470642,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.2147,
      "step": 1100
    },
    {
      "epoch": 1.48,
      "grad_norm": 2.222092628479004,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.2283,
      "step": 1110
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.711920440196991,
      "learning_rate": 1.27e-05,
      "loss": 0.2346,
      "step": 1120
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 10.149999618530273,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.2367,
      "step": 1130
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.0400125980377197,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.2436,
      "step": 1140
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 5.753580570220947,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.2365,
      "step": 1150
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 3.3848373889923096,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.2245,
      "step": 1160
    },
    {
      "epoch": 1.56,
      "grad_norm": 2.5908384323120117,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.2099,
      "step": 1170
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 3.3913581371307373,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.2213,
      "step": 1180
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.2927775382995605,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.2193,
      "step": 1190
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.9210102558135986,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.2365,
      "step": 1200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.9897817969322205,
      "learning_rate": 9.7e-06,
      "loss": 0.226,
      "step": 1210
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 6.450289726257324,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.2355,
      "step": 1220
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 6.098544120788574,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.2411,
      "step": 1230
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 1.9370803833007812,
      "learning_rate": 8.7e-06,
      "loss": 0.2287,
      "step": 1240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.8568286895751953,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.2191,
      "step": 1250
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.3405849933624268,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.2284,
      "step": 1260
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 3.908432722091675,
      "learning_rate": 7.7e-06,
      "loss": 0.2231,
      "step": 1270
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 1.77003014087677,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.2312,
      "step": 1280
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.4037235975265503,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.2159,
      "step": 1290
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.8559733629226685,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.2224,
      "step": 1300
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 4.354764938354492,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.2213,
      "step": 1310
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.8883306980133057,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.229,
      "step": 1320
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.5147481560707092,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.2247,
      "step": 1330
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 1.9762017726898193,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.2274,
      "step": 1340
    },
    {
      "epoch": 1.8,
      "grad_norm": 4.558862686157227,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.2253,
      "step": 1350
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 1.9291208982467651,
      "learning_rate": 4.7e-06,
      "loss": 0.2259,
      "step": 1360
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.8000532984733582,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.2294,
      "step": 1370
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 3.5904998779296875,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.223,
      "step": 1380
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 2.3912858963012695,
      "learning_rate": 3.7e-06,
      "loss": 0.2318,
      "step": 1390
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.924832820892334,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.2184,
      "step": 1400
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.210777521133423,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.2089,
      "step": 1410
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 2.7229273319244385,
      "learning_rate": 2.7e-06,
      "loss": 0.2106,
      "step": 1420
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1.4958528280258179,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.2283,
      "step": 1430
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.3536911010742188,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.2086,
      "step": 1440
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 2.2595102787017822,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.2154,
      "step": 1450
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 3.507810115814209,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.2167,
      "step": 1460
    },
    {
      "epoch": 1.96,
      "grad_norm": 4.782901287078857,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.2282,
      "step": 1470
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 1.720602035522461,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.222,
      "step": 1480
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 6.479172229766846,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.231,
      "step": 1490
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.218886375427246,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 0.2269,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 13502073600000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
