{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 50,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.20804765820503235,
      "learning_rate": 4.97e-05,
      "loss": 0.3234,
      "step": 10
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.21008987724781036,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.3768,
      "step": 20
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.5240922570228577,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.3744,
      "step": 30
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.0738876685500145,
      "learning_rate": 4.87e-05,
      "loss": 0.3464,
      "step": 40
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.3785801827907562,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.3585,
      "step": 50
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.3946889340877533,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.3312,
      "step": 60
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.16443468630313873,
      "learning_rate": 4.77e-05,
      "loss": 0.3515,
      "step": 70
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.1103757694363594,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.3584,
      "step": 80
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.1427457332611084,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.3755,
      "step": 90
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.18602783977985382,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.3724,
      "step": 100
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.10533648729324341,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.3153,
      "step": 110
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.20618973672389984,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.3751,
      "step": 120
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.26655280590057373,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.3526,
      "step": 130
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.17972415685653687,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.3107,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2608450651168823,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.3259,
      "step": 150
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.30230316519737244,
      "learning_rate": 4.47e-05,
      "loss": 0.3568,
      "step": 160
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.1699516773223877,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.3866,
      "step": 170
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4852834939956665,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.3396,
      "step": 180
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.13294893503189087,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.3343,
      "step": 190
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.0728301852941513,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.3822,
      "step": 200
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.26432928442955017,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.3428,
      "step": 210
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.05053230747580528,
      "learning_rate": 4.27e-05,
      "loss": 0.297,
      "step": 220
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.07512380927801132,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.3074,
      "step": 230
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.2553453743457794,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.3386,
      "step": 240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.3433367908000946,
      "learning_rate": 4.17e-05,
      "loss": 0.3767,
      "step": 250
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.04353034868836403,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.3495,
      "step": 260
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.29118144512176514,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.3693,
      "step": 270
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.10257112234830856,
      "learning_rate": 4.07e-05,
      "loss": 0.3563,
      "step": 280
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.09549722820520401,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.3495,
      "step": 290
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.13498206436634064,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.3298,
      "step": 300
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.1938323974609375,
      "learning_rate": 3.97e-05,
      "loss": 0.3161,
      "step": 310
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.16945905983448029,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.3524,
      "step": 320
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.051182061433792114,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.3342,
      "step": 330
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.1347408890724182,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.3384,
      "step": 340
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.26716452836990356,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.3501,
      "step": 350
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.2242177277803421,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.3949,
      "step": 360
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.15154843032360077,
      "learning_rate": 3.77e-05,
      "loss": 0.3611,
      "step": 370
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.47890958189964294,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.3555,
      "step": 380
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.049408700317144394,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.3676,
      "step": 390
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.23230226337909698,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.368,
      "step": 400
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.23650379478931427,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.3577,
      "step": 410
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.12188691645860672,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.312,
      "step": 420
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.3692357540130615,
      "learning_rate": 3.57e-05,
      "loss": 0.3678,
      "step": 430
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.036718036979436874,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.3721,
      "step": 440
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.22157244384288788,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.378,
      "step": 450
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.15453694760799408,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.3342,
      "step": 460
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.20011712610721588,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.3414,
      "step": 470
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.16345246136188507,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.3269,
      "step": 480
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.2623983919620514,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.3836,
      "step": 490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.19198134541511536,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.319,
      "step": 500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.33866026997566223,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.3505,
      "step": 510
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.40158137679100037,
      "learning_rate": 3.27e-05,
      "loss": 0.3659,
      "step": 520
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.2817365825176239,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.3509,
      "step": 530
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.12148017436265945,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.3404,
      "step": 540
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.10963612794876099,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.3634,
      "step": 550
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.14588476717472076,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.3776,
      "step": 560
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.09177979826927185,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.3533,
      "step": 570
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.11872810870409012,
      "learning_rate": 3.07e-05,
      "loss": 0.3618,
      "step": 580
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.2501681447029114,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.3176,
      "step": 590
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07927806675434113,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.3774,
      "step": 600
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.0530078262090683,
      "learning_rate": 2.97e-05,
      "loss": 0.3198,
      "step": 610
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.31124210357666016,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.396,
      "step": 620
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.16913668811321259,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.3549,
      "step": 630
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.15136729180812836,
      "learning_rate": 2.87e-05,
      "loss": 0.3462,
      "step": 640
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.030703768134117126,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.2663,
      "step": 650
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.10082162171602249,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.3269,
      "step": 660
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.06988610327243805,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.3269,
      "step": 670
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.27468374371528625,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.3421,
      "step": 680
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.06209224462509155,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.3427,
      "step": 690
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.24584394693374634,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.3786,
      "step": 700
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.07054383307695389,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.3338,
      "step": 710
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.2394031137228012,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.4107,
      "step": 720
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.2704256772994995,
      "learning_rate": 2.57e-05,
      "loss": 0.3621,
      "step": 730
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.0810626894235611,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.3556,
      "step": 740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.08448535948991776,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.3376,
      "step": 750
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.12099333852529526,
      "learning_rate": 2.47e-05,
      "loss": 0.3217,
      "step": 760
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.24847838282585144,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.3288,
      "step": 770
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.3063310384750366,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.3852,
      "step": 780
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.3282199501991272,
      "learning_rate": 2.37e-05,
      "loss": 0.317,
      "step": 790
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.17381072044372559,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.3201,
      "step": 800
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.09897565841674805,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.3614,
      "step": 810
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.19119828939437866,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.3697,
      "step": 820
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.2657073736190796,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.3507,
      "step": 830
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.22018037736415863,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.3204,
      "step": 840
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.08769708126783371,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.376,
      "step": 850
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.28197839856147766,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.4071,
      "step": 860
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.16064754128456116,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.3889,
      "step": 870
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.17131632566452026,
      "learning_rate": 2.07e-05,
      "loss": 0.3487,
      "step": 880
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.4428376853466034,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.3637,
      "step": 890
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0916486531496048,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.3903,
      "step": 900
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.14115507900714874,
      "learning_rate": 1.97e-05,
      "loss": 0.3594,
      "step": 910
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.040434591472148895,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.3113,
      "step": 920
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.24709300696849823,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.3264,
      "step": 930
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.15961791574954987,
      "learning_rate": 1.87e-05,
      "loss": 0.3306,
      "step": 940
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.1600950062274933,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.3598,
      "step": 950
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.15892180800437927,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.3063,
      "step": 960
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.007497300393879414,
      "learning_rate": 1.77e-05,
      "loss": 0.3029,
      "step": 970
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.26492008566856384,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.3834,
      "step": 980
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.17302101850509644,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.3591,
      "step": 990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.11048299819231033,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.3257,
      "step": 1000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.20792235434055328,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.331,
      "step": 1010
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.06537515670061111,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.3594,
      "step": 1020
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.25609758496284485,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.3497,
      "step": 1030
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.05893760919570923,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.3279,
      "step": 1040
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.20523701608181,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.376,
      "step": 1050
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.163052037358284,
      "learning_rate": 1.47e-05,
      "loss": 0.3151,
      "step": 1060
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.11286402493715286,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.3476,
      "step": 1070
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.05229441076517105,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.3523,
      "step": 1080
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.3525553345680237,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.3473,
      "step": 1090
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.07798776775598526,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.3289,
      "step": 1100
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.18672557175159454,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.3232,
      "step": 1110
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.04282736778259277,
      "learning_rate": 1.27e-05,
      "loss": 0.353,
      "step": 1120
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.2517593801021576,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.3424,
      "step": 1130
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.05952394753694534,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.3722,
      "step": 1140
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.13107268512248993,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.3392,
      "step": 1150
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.09410877525806427,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.3667,
      "step": 1160
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.06833641976118088,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.3117,
      "step": 1170
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.31687358021736145,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.363,
      "step": 1180
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.11560270190238953,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.3085,
      "step": 1190
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.1914394050836563,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.4151,
      "step": 1200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.19777736067771912,
      "learning_rate": 9.7e-06,
      "loss": 0.3494,
      "step": 1210
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.14079762995243073,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.3448,
      "step": 1220
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.14981307089328766,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.4089,
      "step": 1230
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.1831103265285492,
      "learning_rate": 8.7e-06,
      "loss": 0.3487,
      "step": 1240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.18462389707565308,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.3683,
      "step": 1250
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.06803616881370544,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.3622,
      "step": 1260
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.1907639503479004,
      "learning_rate": 7.7e-06,
      "loss": 0.3369,
      "step": 1270
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.10675928741693497,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.315,
      "step": 1280
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.0836501270532608,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.3411,
      "step": 1290
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.05390331149101257,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.3541,
      "step": 1300
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.1075771152973175,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.3749,
      "step": 1310
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.12198067456483841,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.3104,
      "step": 1320
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.0725286602973938,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.3594,
      "step": 1330
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.1262086182832718,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.3648,
      "step": 1340
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.15088967978954315,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.3552,
      "step": 1350
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.2786253094673157,
      "learning_rate": 4.7e-06,
      "loss": 0.3831,
      "step": 1360
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.008945148438215256,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.3474,
      "step": 1370
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.09959220141172409,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.3621,
      "step": 1380
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.08021527528762817,
      "learning_rate": 3.7e-06,
      "loss": 0.3735,
      "step": 1390
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.1770886927843094,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.3661,
      "step": 1400
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.11166387796401978,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.3033,
      "step": 1410
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.24121998250484467,
      "learning_rate": 2.7e-06,
      "loss": 0.3425,
      "step": 1420
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.07568591088056564,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.3804,
      "step": 1430
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.1185162216424942,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.3438,
      "step": 1440
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.15328437089920044,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.3432,
      "step": 1450
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.23358489573001862,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.3155,
      "step": 1460
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.07275856286287308,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.3559,
      "step": 1470
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.08763571083545685,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.3343,
      "step": 1480
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.2793647050857544,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.382,
      "step": 1490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.18737588822841644,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 0.3445,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 13502073600000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
